"""
ìì—°ì–´ ê²€ìƒ‰ API ë¼ìš°í„°

FastAPI 0.104+ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ê³¼ Pydantic v2ë¥¼ ì‚¬ìš©í•œ
ìì—°ì–´ ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸ ë° WebSocket ì‹¤ì‹œê°„ ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import logging
import asyncio
import json
from typing import List, Dict, Any, Optional, Union, Annotated, AsyncGenerator
from datetime import datetime
from contextlib import asynccontextmanager

from fastapi import (
    APIRouter, Depends, HTTPException, WebSocket, WebSocketDisconnect,
    Query, Path, Body, BackgroundTasks, Request, Response, status
)
from fastapi.responses import StreamingResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, Field, ConfigDict, field_validator, computed_field
from pydantic.types import StringConstraints
from pydantic_core import ValidationError

from app.services.lcel_sql_pipeline import (
    lcel_sql_pipeline,
    EnhancedSQLGenerationRequest,
    EnhancedSQLPipelineResponse,
    ExecutionStrategy,
    RetryConfig
)
from app.services.nl_search_service import (
    nl_search_service,
    NLSearchRequest,
    NLSearchResponse
)
from app.services.search_cache_service import search_cache_service
from app.services.search_formatter import search_formatter, HighlightOptions
from app.utils.langsmith_config import trace_llm_call
from app.database import read_only_db_manager

logger = logging.getLogger(__name__)

# FastAPI 0.104+ ìŠ¤íƒ€ì¼ ë¼ìš°í„° ìƒì„±
router = APIRouter(
    prefix="/api/search",
    tags=["Natural Language Search"],
    responses={
        404: {"description": "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"},
        422: {"description": "ì…ë ¥ ë°ì´í„° ê²€ì¦ ì˜¤ë¥˜"},
        500: {"description": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜"}
    }
)

# ë³´ì•ˆ ìŠ¤í‚¤ë§ˆ ì •ì˜ (ì„ íƒì )
security = HTTPBearer(auto_error=False)

# Pydantic v2 ìŠ¤íƒ€ì¼ íƒ€ì… ì •ì˜
QueryString = Annotated[str, StringConstraints(min_length=1, max_length=1000, strip_whitespace=True)]
ContextData = Annotated[Dict[str, Any], Field(default_factory=dict)]
LimitValue = Annotated[int, Field(ge=1, le=1000, default=100)]


# Request/Response ëª¨ë¸ë“¤ (Pydantic v2)
class SearchIntent(BaseModel):
    """ê²€ìƒ‰ ì˜ë„ ì •ë³´"""
    model_config = ConfigDict(
        str_strip_whitespace=True,
        validate_assignment=True,
        extra="forbid"
    )
    
    intent_type: str = Field(..., description="ê²€ìƒ‰ ì˜ë„ íƒ€ì…", examples=["customer_search", "data_analysis"])
    confidence: float = Field(..., ge=0.0, le=1.0, description="ì‹ ë¢°ë„ ì ìˆ˜")
    keywords: List[str] = Field(default_factory=list, description="ì¶”ì¶œëœ í‚¤ì›Œë“œ")
    entities: Dict[str, List[str]] = Field(default_factory=dict, description="ì¶”ì¶œëœ ì—”í‹°í‹°")


class SearchOptions(BaseModel):
    """ê²€ìƒ‰ ì˜µì…˜"""
    model_config = ConfigDict(
        str_strip_whitespace=True,
        validate_assignment=True
    )
    
    strategy: ExecutionStrategy = Field(
        default=ExecutionStrategy.LLM_FIRST,
        description="SQL ìƒì„± ì „ëµ",
        examples=["llm_first", "rule_first", "hybrid"]
    )
    enable_streaming: bool = Field(
        default=False,
        description="ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í™œì„±í™”"
    )
    include_explanation: bool = Field(
        default=True,
        description="ì¿¼ë¦¬ ì„¤ëª… í¬í•¨ ì—¬ë¶€"
    )
    timeout_seconds: float = Field(
        default=30.0,
        ge=1.0,
        le=120.0,
        description="íƒ€ì„ì•„ì›ƒ (ì´ˆ)"
    )
    
    @field_validator('strategy', mode='before')
    @classmethod
    def validate_strategy(cls, v):
        """ì „ëµ ê°’ ê²€ì¦"""
        if isinstance(v, str):
            try:
                return ExecutionStrategy(v)
            except ValueError:
                raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì „ëµ: {v}")
        return v


class NaturalLanguageSearchRequest(BaseModel):
    """ìì—°ì–´ ê²€ìƒ‰ ìš”ì²­"""
    model_config = ConfigDict(
        str_strip_whitespace=True,
        validate_assignment=True,
        json_schema_extra={
            "example": {
                "query": "30ëŒ€ ê³ ê°ë“¤ì˜ í‰ê·  ë³´í—˜ë£Œë¥¼ ì§€ì—­ë³„ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”",
                "context": {"department": "analytics", "user_level": "advanced"},
                "options": {
                    "strategy": "hybrid",
                    "include_explanation": True,
                    "timeout_seconds": 45.0
                },
                "limit": 100
            }
        }
    )
    
    query: QueryString = Field(
        ..., 
        description="ìì—°ì–´ ê²€ìƒ‰ ì¿¼ë¦¬",
        examples=["30ëŒ€ ê³ ê° ëª©ë¡", "ì§€ë‚œë‹¬ ê°€ì…í•œ ê³ ê° ìˆ˜"]
    )
    context: ContextData = Field(
        default_factory=dict,
        description="ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì •ë³´",
        examples=[{"department": "sales", "region": "seoul"}]
    )
    options: SearchOptions = Field(
        default_factory=SearchOptions,
        description="ê²€ìƒ‰ ì˜µì…˜"
    )
    limit: LimitValue = Field(
        default=100,
        description="ê²°ê³¼ ì œí•œ ìˆ˜"
    )
    
    @field_validator('context')
    @classmethod
    def validate_context(cls, v):
        """ì»¨í…ìŠ¤íŠ¸ ê²€ì¦"""
        if v and len(json.dumps(v)) > 10000:  # 10KB ì œí•œ
            raise ValueError("ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤")
        return v


class SearchExecution(BaseModel):
    """ê²€ìƒ‰ ì‹¤í–‰ ì •ë³´"""
    model_config = ConfigDict(validate_assignment=True)
    
    sql_query: str = Field(..., description="ì‹¤í–‰ëœ SQL ì¿¼ë¦¬")
    parameters: Dict[str, Any] = Field(default_factory=dict, description="SQL íŒŒë¼ë¯¸í„°")
    execution_time_ms: float = Field(..., ge=0, description="ì‹¤í–‰ ì‹œê°„ (ë°€ë¦¬ì´ˆ)")
    rows_affected: int = Field(..., ge=0, description="ì˜í–¥ë°›ì€ í–‰ ìˆ˜")
    strategy_used: str = Field(..., description="ì‚¬ìš©ëœ ì „ëµ")


class NaturalLanguageSearchResponse(BaseModel):
    """ìì—°ì–´ ê²€ìƒ‰ ì‘ë‹µ"""
    model_config = ConfigDict(
        validate_assignment=True,
        json_schema_extra={
            "example": {
                "request_id": "req_123456789",
                "query": "30ëŒ€ ê³ ê°ë“¤ì˜ í‰ê·  ë³´í—˜ë£Œ",
                "intent": {
                    "intent_type": "data_analysis",
                    "confidence": 0.92,
                    "keywords": ["30ëŒ€", "ê³ ê°", "í‰ê· ", "ë³´í—˜ë£Œ"],
                    "entities": {"age_group": ["30ëŒ€"], "metric": ["í‰ê· ", "ë³´í—˜ë£Œ"]}
                },
                "execution": {
                    "sql_query": "SELECT AVG(premium) FROM customers WHERE age BETWEEN 30 AND 39",
                    "parameters": {},
                    "execution_time_ms": 156.7,
                    "rows_affected": 1,
                    "strategy_used": "llm_first"
                },
                "data": [{"avg_premium": 125000}],
                "total_rows": 1,
                "success": True,
                "timestamp": "2024-01-15T10:30:00Z"
            }
        }
    )
    
    request_id: str = Field(..., description="ìš”ì²­ ê³ ìœ  ID")
    query: str = Field(..., description="ì›ë³¸ ì¿¼ë¦¬")
    intent: SearchIntent = Field(..., description="ë¶„ì„ëœ ê²€ìƒ‰ ì˜ë„")
    execution: SearchExecution = Field(..., description="ê²€ìƒ‰ ì‹¤í–‰ ì •ë³´")
    data: List[Dict[str, Any]] = Field(default_factory=list, description="ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°")
    total_rows: int = Field(..., ge=0, description="ì´ ê²°ê³¼ í–‰ ìˆ˜")
    success: bool = Field(..., description="ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€")
    error_message: Optional[str] = Field(None, description="ì˜¤ë¥˜ ë©”ì‹œì§€ (ì‹¤íŒ¨ì‹œ)")
    timestamp: datetime = Field(default_factory=datetime.now, description="ì‘ë‹µ ì‹œê°„")
    
    @computed_field
    @property
    def has_data(self) -> bool:
        """ë°ì´í„° ì¡´ì¬ ì—¬ë¶€"""
        return len(self.data) > 0
    
    @computed_field
    @property
    def execution_summary(self) -> str:
        """ì‹¤í–‰ ìš”ì•½"""
        return f"{self.execution.strategy_used} ì „ëµìœ¼ë¡œ {self.execution.execution_time_ms:.1f}msì— {self.total_rows}í–‰ ê²€ìƒ‰"


class StreamingSearchEvent(BaseModel):
    """ìŠ¤íŠ¸ë¦¬ë° ê²€ìƒ‰ ì´ë²¤íŠ¸"""
    model_config = ConfigDict(validate_assignment=True)
    
    event_type: str = Field(..., description="ì´ë²¤íŠ¸ íƒ€ì…")
    timestamp: datetime = Field(default_factory=datetime.now, description="ì´ë²¤íŠ¸ ì‹œê°„")
    data: Dict[str, Any] = Field(default_factory=dict, description="ì´ë²¤íŠ¸ ë°ì´í„°")
    progress: Optional[float] = Field(None, ge=0.0, le=1.0, description="ì§„í–‰ë¥  (0-1)")


# ì˜ì¡´ì„± ì£¼ì… í•¨ìˆ˜ë“¤ (FastAPI 0.104+ íŒ¨í„´)
async def get_search_context(
    request: Request,
    user_agent: Annotated[Optional[str], Depends(lambda r: r.headers.get("user-agent"))] = None,
    x_request_id: Annotated[Optional[str], Depends(lambda r: r.headers.get("x-request-id"))] = None
) -> Dict[str, Any]:
    """ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    return {
        "client_ip": getattr(request.client, 'host', 'unknown'),
        "user_agent": user_agent,
        "request_id": x_request_id or f"req_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}",
        "timestamp": datetime.now().isoformat()
    }


async def get_auth_info(
    credentials: Annotated[Optional[HTTPAuthorizationCredentials], Depends(security)] = None
) -> Dict[str, Any]:
    """ì¸ì¦ ì •ë³´ ì¶”ì¶œ (ì„ íƒì )"""
    if credentials:
        # ì—¬ê¸°ì„œ ì‹¤ì œ í† í° ê²€ì¦ ë¡œì§ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        return {"authenticated": True, "token": credentials.credentials}
    return {"authenticated": False}


async def validate_search_permissions(
    auth_info: Annotated[Dict[str, Any], Depends(get_auth_info)],
    request_data: Optional[Dict[str, Any]] = None
) -> bool:
    """ê²€ìƒ‰ ê¶Œí•œ ê²€ì¦"""
    # ì‹¤ì œ ê¶Œí•œ ê²€ì¦ ë¡œì§ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    # í˜„ì¬ëŠ” ëª¨ë“  ìš”ì²­ì„ í—ˆìš©
    return True


# WebSocket ì—°ê²° ê´€ë¦¬ì
class WebSocketManager:
    """WebSocket ì—°ê²° ê´€ë¦¬ì"""
    
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
        self.connection_metadata: Dict[str, Dict[str, Any]] = {}
    
    async def connect(self, websocket: WebSocket, client_id: str, metadata: Dict[str, Any] = None):
        """WebSocket ì—°ê²° ìˆ˜ë½"""
        await websocket.accept()
        self.active_connections[client_id] = websocket
        self.connection_metadata[client_id] = metadata or {}
        logger.info(f"WebSocket í´ë¼ì´ì–¸íŠ¸ ì—°ê²°: {client_id}")
    
    def disconnect(self, client_id: str):
        """WebSocket ì—°ê²° í•´ì œ"""
        if client_id in self.active_connections:
            del self.active_connections[client_id]
        if client_id in self.connection_metadata:
            del self.connection_metadata[client_id]
        logger.info(f"WebSocket í´ë¼ì´ì–¸íŠ¸ í•´ì œ: {client_id}")
    
    async def send_personal_message(self, message: Dict[str, Any], client_id: str):
        """ê°œë³„ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë©”ì‹œì§€ ì „ì†¡"""
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_text(json.dumps(message, default=str))
            except Exception as e:
                logger.error(f"WebSocket ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨ ({client_id}): {e}")
                self.disconnect(client_id)
    
    async def broadcast(self, message: Dict[str, Any]):
        """ëª¨ë“  ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        disconnected_clients = []
        for client_id, websocket in self.active_connections.items():
            try:
                await websocket.send_text(json.dumps(message, default=str))
            except Exception as e:
                logger.error(f"ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì‹¤íŒ¨ ({client_id}): {e}")
                disconnected_clients.append(client_id)
        
        # ì—°ê²° ì‹¤íŒ¨í•œ í´ë¼ì´ì–¸íŠ¸ ì œê±°
        for client_id in disconnected_clients:
            self.disconnect(client_id)


# ì „ì—­ WebSocket ê´€ë¦¬ì
websocket_manager = WebSocketManager()


# API ì—”ë“œí¬ì¸íŠ¸ë“¤
@router.post(
    "/natural-language",
    response_model=NaturalLanguageSearchResponse,
    status_code=status.HTTP_200_OK,
    summary="ìì—°ì–´ ê²€ìƒ‰",
    description="""
    ìì—°ì–´ ì¿¼ë¦¬ë¥¼ SQLë¡œ ë³€í™˜í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    ## ì£¼ìš” ê¸°ëŠ¥
    - í•œêµ­ì–´ ìì—°ì–´ ì¿¼ë¦¬ ì§€ì›
    - ë‹¤ì–‘í•œ SQL ìƒì„± ì „ëµ
    - ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    - ìë™ ì¿¼ë¦¬ ìµœì í™”
    
    ## ì „ëµ ì„¤ëª…
    - **llm_first**: LLM ìš°ì„ , ì‹¤íŒ¨ì‹œ ê·œì¹™ ê¸°ë°˜ (ê¸°ë³¸ê°’)
    - **rule_first**: ê·œì¹™ ê¸°ë°˜ ìš°ì„ , ì‹¤íŒ¨ì‹œ LLM
    - **hybrid**: ë³‘ë ¬ ì‹¤í–‰ í›„ ìµœì  ê²°ê³¼ ì„ íƒ
    - **llm_only**: LLMë§Œ ì‚¬ìš©
    - **rule_only**: ê·œì¹™ ê¸°ë°˜ë§Œ ì‚¬ìš©
    """,
    response_description="ê²€ìƒ‰ ê²°ê³¼ ë° ì‹¤í–‰ ì •ë³´"
)
@trace_llm_call("natural_language_search_api")
async def natural_language_search(
    request_data: Annotated[NaturalLanguageSearchRequest, Body(
        ...,
        examples=[
            {
                "query": "30ëŒ€ ê³ ê°ë“¤ì˜ í‰ê·  ë³´í—˜ë£Œë¥¼ ì§€ì—­ë³„ë¡œ ë³´ì—¬ì£¼ì„¸ìš”",
                "context": {"department": "analytics"},
                "options": {"strategy": "llm_first", "timeout_seconds": 30.0},
                "limit": 100
            },
            {
                "query": "ìµœê·¼ 1ê°œì›”ê°„ ê°€ì…í•œ ê³ ê° ìˆ˜",
                "options": {"strategy": "rule_first", "include_explanation": False},
                "limit": 50
            }
        ]
    )],
    search_context: Annotated[Dict[str, Any], Depends(get_search_context)],
    auth_info: Annotated[Dict[str, Any], Depends(get_auth_info)],
    has_permission: Annotated[bool, Depends(validate_search_permissions)],
    background_tasks: BackgroundTasks,
    use_cache: Annotated[bool, Query(description="ìºì‹œ ì‚¬ìš© ì—¬ë¶€")] = True,
    enable_highlighting: Annotated[bool, Query(description="ê²€ìƒ‰ì–´ í•˜ì´ë¼ì´íŒ… í™œì„±í™”")] = True
) -> NaturalLanguageSearchResponse:
    """ìì—°ì–´ ê²€ìƒ‰ ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    
    if not has_permission:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="ê²€ìƒ‰ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤"
        )
    
    request_id = search_context["request_id"]
    start_time = datetime.now()
    
    try:
        logger.info(f"ğŸ” ìì—°ì–´ ê²€ìƒ‰ ì‹œì‘ [{request_id}]: {request_data.query} (ìºì‹œ={use_cache}, í•˜ì´ë¼ì´íŒ…={enable_highlighting})")
        
        # 1. ìºì‹œ ì¡°íšŒ (ì‚¬ìš© ì„¤ì • ì‹œ)
        if use_cache:
            cache_context = {
                "context": request_data.context,
                "auth_info": {k: v for k, v in auth_info.items() if k != "token"},  # í† í°ì€ ìºì‹œ í‚¤ì—ì„œ ì œì™¸
                "limit": request_data.limit
            }
            
            cached_result = await search_cache_service.get_cached_result(
                query=request_data.query,
                context=cache_context,
                options=request_data.options.model_dump()
            )
            
            if cached_result:
                logger.info(f"âœ… ìºì‹œ íˆíŠ¸ [{request_id}]: ìºì‹œëœ ê²°ê³¼ ë°˜í™˜")
                
                # ìºì‹œëœ ì‘ë‹µì— ìš”ì²­ ID ì—…ë°ì´íŠ¸
                cached_result.update({
                    "request_id": request_id,
                    "timestamp": datetime.now().isoformat()
                })
                
                # í•˜ì´ë¼ì´íŒ… ì²˜ë¦¬ (ìºì‹œëœ ê²°ê³¼ì—ë„ ì ìš©)
                if enable_highlighting and cached_result.get("data"):
                    highlight_options = HighlightOptions(case_sensitive=False, whole_words_only=False)
                    cached_result["data"] = search_formatter.highlight_search_results(
                        cached_result["data"], 
                        request_data.query,
                        highlight_options
                    )
                
                return NaturalLanguageSearchResponse(**cached_result)
        
        # 2. ìºì‹œ ë¯¸ìŠ¤ - ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰
        logger.info(f"âŒ ìºì‹œ ë¯¸ìŠ¤ [{request_id}]: ìƒˆë¡œìš´ ê²€ìƒ‰ ìˆ˜í–‰")
        
        # LCEL íŒŒì´í”„ë¼ì¸ ìš”ì²­ ìƒì„±
        pipeline_request = EnhancedSQLGenerationRequest(
            query=request_data.query,
            context={**request_data.context, **search_context, **auth_info},
            strategy=request_data.options.strategy,
            enable_streaming=request_data.options.enable_streaming,
            timeout_seconds=request_data.options.timeout_seconds
        )
        
        # SQL ìƒì„± ë° ì‹¤í–‰
        pipeline_result = await lcel_sql_pipeline.generate_sql(pipeline_request)
        
        if not pipeline_result.success:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"SQL ìƒì„± ì‹¤íŒ¨: {pipeline_result.error_message}"
            )
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì‹¤í–‰
        try:
            execution_start = datetime.now()
            db_results = await read_only_db_manager.execute_query_with_limit(
                pipeline_result.sql_result.sql,
                pipeline_result.sql_result.parameters,
                limit=request_data.limit
            )
            execution_time = (datetime.now() - execution_start).total_seconds() * 1000
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            data = []
            if db_results:
                columns = db_results[0]._fields if hasattr(db_results[0], '_fields') else []
                data = [dict(zip(columns, row)) for row in db_results]
            
            logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ [{request_id}]: {len(data)}í–‰")
            
        except Exception as db_error:
            logger.error(f"âŒ DB ì‹¤í–‰ ì‹¤íŒ¨ [{request_id}]: {db_error}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"ë°ì´í„°ë² ì´ìŠ¤ ì‹¤í–‰ ì˜¤ë¥˜: {str(db_error)}"
            )
        
        # 3. ê²°ê³¼ í¬ë§·íŒ… (í•˜ì´ë¼ì´íŒ…)
        formatted_data = data
        if enable_highlighting and data:
            highlight_options = HighlightOptions(case_sensitive=False, whole_words_only=False)
            formatted_data = search_formatter.highlight_search_results(
                data, 
                request_data.query,
                highlight_options
            )
            logger.debug(f"í•˜ì´ë¼ì´íŒ… ì²˜ë¦¬ ì™„ë£Œ [{request_id}]: {len(formatted_data)}í–‰")
        
        # 4. ì‘ë‹µ ìƒì„±
        response_data = {
            "request_id": request_id,
            "query": request_data.query,
            "intent": {
                "intent_type": pipeline_result.intent_analysis.get("query_type", {}).get("main_type", "unknown"),
                "confidence": pipeline_result.intent_analysis.get("query_type", {}).get("confidence", 0.0),
                "keywords": pipeline_result.intent_analysis.get("intent_keywords", []),
                "entities": pipeline_result.intent_analysis.get("entities", {})
            },
            "execution": {
                "sql_query": pipeline_result.sql_result.sql,
                "parameters": pipeline_result.sql_result.parameters,
                "execution_time_ms": execution_time,
                "rows_affected": len(data),
                "strategy_used": pipeline_result.sql_result.generation_method
            },
            "data": formatted_data,
            "total_rows": len(data),
            "success": True,
            "formatting_applied": enable_highlighting,
            "cache_info": {
                "cached": False,
                "cache_enabled": use_cache
            }
        }
        
        response = NaturalLanguageSearchResponse(**response_data)
        
        # 5. ìºì‹œ ì €ì¥ (ë°±ê·¸ë¼ìš´ë“œ)
        if use_cache:
            cache_context = {
                "context": request_data.context,
                "auth_info": {k: v for k, v in auth_info.items() if k != "token"},
                "limit": request_data.limit
            }
            background_tasks.add_task(
                _cache_search_result,
                request_data.query,
                response_data,
                cache_context,
                request_data.options.model_dump(),
                int(execution_time)
            )
        
        # 6. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë©”íŠ¸ë¦­ ì €ì¥
        background_tasks.add_task(
            _log_search_metrics,
            request_id,
            request_data.query,
            response.execution.strategy_used,
            response.total_rows,
            (datetime.now() - start_time).total_seconds()
        )
        
        return response
        
    except HTTPException:
        raise
    except ValidationError as ve:
        logger.error(f"âŒ ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨ [{request_id}]: {ve}")
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=f"ì…ë ¥ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {str(ve)}"
        )
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨ [{request_id}]: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {str(e)}"
        )


@router.get(
    "/strategies",
    response_model=Dict[str, Any],
    summary="ê²€ìƒ‰ ì „ëµ ëª©ë¡",
    description="ì‚¬ìš© ê°€ëŠ¥í•œ ìì—°ì–´ ê²€ìƒ‰ ì „ëµë“¤ê³¼ ê°ê°ì˜ íŠ¹ì§•ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
)
async def get_search_strategies() -> Dict[str, Any]:
    """ê²€ìƒ‰ ì „ëµ ëª©ë¡ ì¡°íšŒ"""
    
    strategies = {
        "llm_first": {
            "name": "LLM ìš°ì„ ",
            "description": "LLMì„ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì‹¤íŒ¨ì‹œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ Fallback",
            "accuracy": "ë†’ìŒ",
            "speed": "ì¤‘ê°„",
            "cost": "ì¤‘ê°„",
            "recommended_for": ["ë³µì¡í•œ ì¿¼ë¦¬", "ë†’ì€ ì •í™•ë„ê°€ í•„ìš”í•œ ê²½ìš°", "ì¼ë°˜ì ì¸ ì‚¬ìš©"]
        },
        "rule_first": {
            "name": "ê·œì¹™ ìš°ì„ ",
            "description": "ê·œì¹™ ê¸°ë°˜ì„ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì‹¤íŒ¨ì‹œ LLMìœ¼ë¡œ Fallback",
            "accuracy": "ì¤‘ê°„",
            "speed": "ë¹ ë¦„",
            "cost": "ë‚®ìŒ",
            "recommended_for": ["ê°„ë‹¨í•œ ì¿¼ë¦¬", "ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš°", "ì •í˜•í™”ëœ íŒ¨í„´"]
        },
        "hybrid": {
            "name": "í•˜ì´ë¸Œë¦¬ë“œ",
            "description": "LLMê³¼ ê·œì¹™ ê¸°ë°˜ì„ ë³‘ë ¬ ì‹¤í–‰ í›„ ìµœì  ê²°ê³¼ ì„ íƒ",
            "accuracy": "ìµœê³ ",
            "speed": "ëŠë¦¼",
            "cost": "ë†’ìŒ",
            "recommended_for": ["ì¤‘ìš”í•œ ì¿¼ë¦¬", "ìµœê³  í’ˆì§ˆì´ í•„ìš”í•œ ê²½ìš°", "ì •í™•ë„ ìš°ì„ "]
        },
        "llm_only": {
            "name": "LLM ì „ìš©",
            "description": "LLMë§Œ ì‚¬ìš©, Fallback ì—†ìŒ",
            "accuracy": "ë†’ìŒ",
            "speed": "ì¤‘ê°„",
            "cost": "ì¤‘ê°„",
            "recommended_for": ["ì°½ì˜ì  ì¿¼ë¦¬", "LLM ì„±ëŠ¥ í…ŒìŠ¤íŠ¸", "ìƒˆë¡œìš´ íŒ¨í„´ íƒìƒ‰"]
        },
        "rule_only": {
            "name": "ê·œì¹™ ì „ìš©",
            "description": "ê·œì¹™ ê¸°ë°˜ë§Œ ì‚¬ìš©, LLM ì‚¬ìš© ì•ˆí•¨",
            "accuracy": "ë‚®ìŒ",
            "speed": "ìµœê³ ",
            "cost": "ì—†ìŒ",
            "recommended_for": ["ëŒ€ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬", "ë¹„ìš© ìµœì í™”", "ì •í•´ì§„ íŒ¨í„´ë§Œ"]
        }
    }
    
    return {
        "strategies": strategies,
        "default": "llm_first",
        "total_count": len(strategies),
        "recommendation": {
            "general_use": "llm_first",
            "high_performance": "rule_first",
            "best_quality": "hybrid",
            "cost_effective": "rule_only"
        }
    }


@router.get(
    "/health",
    response_model=Dict[str, Any],
    summary="ê²€ìƒ‰ ì„œë¹„ìŠ¤ ìƒíƒœ",
    description="ìì—°ì–´ ê²€ìƒ‰ ì„œë¹„ìŠ¤ì˜ ìƒíƒœì™€ ì˜ì¡´ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤."
)
async def search_health_check() -> Dict[str, Any]:
    """ê²€ìƒ‰ ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬"""
    
    health_status = {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "natural_language_search",
        "version": "2.0.0",
        "components": {}
    }
    
    try:
        # LCEL íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸
        try:
            test_request = EnhancedSQLGenerationRequest(
                query="í…ŒìŠ¤íŠ¸",
                strategy=ExecutionStrategy.RULE_ONLY,
                timeout_seconds=5.0
            )
            await lcel_sql_pipeline.generate_sql(test_request)
            health_status["components"]["lcel_pipeline"] = "healthy"
        except Exception as e:
            health_status["components"]["lcel_pipeline"] = f"unhealthy: {str(e)}"
            health_status["status"] = "degraded"
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸
        try:
            await read_only_db_manager.execute_query_with_limit("SELECT 1", {}, limit=1)
            health_status["components"]["database"] = "healthy"
        except Exception as e:
            health_status["components"]["database"] = f"unhealthy: {str(e)}"
            health_status["status"] = "degraded"
        
        # WebSocket ê´€ë¦¬ì ìƒíƒœ
        health_status["components"]["websocket_manager"] = {
            "status": "healthy",
            "active_connections": len(websocket_manager.active_connections)
        }
        
        return health_status
        
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


# WebSocket ì—”ë“œí¬ì¸íŠ¸
@router.websocket("/stream")
async def websocket_search_stream(
    websocket: WebSocket,
    client_id: Annotated[str, Query(description="í´ë¼ì´ì–¸íŠ¸ ê³ ìœ  ID")] = None
):
    """ì‹¤ì‹œê°„ ê²€ìƒ‰ ìŠ¤íŠ¸ë¦¬ë° WebSocket"""
    
    if not client_id:
        client_id = f"client_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
    
    await websocket_manager.connect(websocket, client_id, {
        "connected_at": datetime.now().isoformat(),
        "type": "search_stream"
    })
    
    try:
        # ì—°ê²° ì„±ê³µ ë©”ì‹œì§€ ì „ì†¡
        await websocket_manager.send_personal_message({
            "event_type": "connection_established",
            "client_id": client_id,
            "timestamp": datetime.now().isoformat(),
            "message": "ì‹¤ì‹œê°„ ê²€ìƒ‰ ìŠ¤íŠ¸ë¦¼ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤."
        }, client_id)
        
        while True:
            try:
                # í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë©”ì‹œì§€ ìˆ˜ì‹ 
                data = await websocket.receive_text()
                message = json.loads(data)
                
                if message.get("type") == "search_request":
                    # ê²€ìƒ‰ ìš”ì²­ ì²˜ë¦¬
                    await _handle_streaming_search(message, client_id)
                elif message.get("type") == "ping":
                    # ì—°ê²° ìƒíƒœ í™•ì¸
                    await websocket_manager.send_personal_message({
                        "event_type": "pong",
                        "timestamp": datetime.now().isoformat()
                    }, client_id)
                else:
                    # ì•Œ ìˆ˜ ì—†ëŠ” ë©”ì‹œì§€ íƒ€ì…
                    await websocket_manager.send_personal_message({
                        "event_type": "error",
                        "message": f"ì•Œ ìˆ˜ ì—†ëŠ” ë©”ì‹œì§€ íƒ€ì…: {message.get('type')}",
                        "timestamp": datetime.now().isoformat()
                    }, client_id)
                    
            except json.JSONDecodeError:
                await websocket_manager.send_personal_message({
                    "event_type": "error",
                    "message": "ì˜ëª»ëœ JSON í˜•ì‹ì…ë‹ˆë‹¤.",
                    "timestamp": datetime.now().isoformat()
                }, client_id)
            except Exception as e:
                logger.error(f"WebSocket ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜ ({client_id}): {e}")
                await websocket_manager.send_personal_message({
                    "event_type": "error",
                    "message": f"ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                    "timestamp": datetime.now().isoformat()
                }, client_id)
                
    except WebSocketDisconnect:
        logger.info(f"WebSocket í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í•´ì œ: {client_id}")
    except Exception as e:
        logger.error(f"WebSocket ì—°ê²° ì˜¤ë¥˜ ({client_id}): {e}")
    finally:
        websocket_manager.disconnect(client_id)


# í—¬í¼ í•¨ìˆ˜ë“¤
async def _handle_streaming_search(message: Dict[str, Any], client_id: str):
    """ìŠ¤íŠ¸ë¦¬ë° ê²€ìƒ‰ ì²˜ë¦¬"""
    try:
        query = message.get("query", "")
        if not query:
            await websocket_manager.send_personal_message({
                "event_type": "error",
                "message": "ê²€ìƒ‰ ì¿¼ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "timestamp": datetime.now().isoformat()
            }, client_id)
            return
        
        # ê²€ìƒ‰ ì‹œì‘ ì•Œë¦¼
        await websocket_manager.send_personal_message({
            "event_type": "search_started",
            "query": query,
            "timestamp": datetime.now().isoformat()
        }, client_id)
        
        # LCEL íŒŒì´í”„ë¼ì¸ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        options = message.get("options", {})
        request = EnhancedSQLGenerationRequest(
            query=query,
            context=message.get("context", {}),
            strategy=ExecutionStrategy(options.get("strategy", "llm_first")),
            enable_streaming=True,
            timeout_seconds=options.get("timeout_seconds", 30.0)
        )
        
        # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬
        async for event in lcel_sql_pipeline.generate_sql_streaming(request):
            streaming_event = StreamingSearchEvent(
                event_type=event.get("type", "update"),
                data=event,
                progress=event.get("progress")
            )
            
            await websocket_manager.send_personal_message(
                streaming_event.model_dump(mode='json'),
                client_id
            )
        
        # ê²€ìƒ‰ ì™„ë£Œ ì•Œë¦¼
        await websocket_manager.send_personal_message({
            "event_type": "search_completed",
            "query": query,
            "timestamp": datetime.now().isoformat()
        }, client_id)
        
    except Exception as e:
        logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ê²€ìƒ‰ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        await websocket_manager.send_personal_message({
            "event_type": "error",
            "message": f"ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }, client_id)


async def _cache_search_result(
    query: str,
    result: Dict[str, Any],
    context: Dict[str, Any],
    options: Dict[str, Any],
    execution_time_ms: int
):
    """ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ì €ì¥ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)"""
    try:
        success = await search_cache_service.cache_search_result(
            query=query,
            result=result,
            context=context,
            options=options,
            execution_time_ms=execution_time_ms,
            ttl_minutes=5  # 5ë¶„ TTL
        )
        
        if success:
            logger.debug(f"âœ… ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ì €ì¥ ì„±ê³µ: {query[:50]}...")
        else:
            logger.warning(f"âŒ ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {query[:50]}...")
            
    except Exception as e:
        logger.error(f"ìºì‹œ ì €ì¥ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹¤íŒ¨: {e}")


async def _log_search_metrics(
    request_id: str,
    query: str,
    strategy: str,
    result_count: int,
    duration_seconds: float
):
    """ê²€ìƒ‰ ë©”íŠ¸ë¦­ ë¡œê¹… (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)"""
    try:
        metrics = {
            "request_id": request_id,
            "query_length": len(query),
            "strategy_used": strategy,
            "result_count": result_count,
            "duration_seconds": duration_seconds,
            "timestamp": datetime.now().isoformat()
        }
        
        # ì‹¤ì œ ë©”íŠ¸ë¦­ ì €ì¥ì†Œì— ì €ì¥ (ì˜ˆ: ë°ì´í„°ë² ì´ìŠ¤, ë¡œê·¸ íŒŒì¼ ë“±)
        logger.info(f"ğŸ“Š ê²€ìƒ‰ ë©”íŠ¸ë¦­: {metrics}")
        
    except Exception as e:
        logger.error(f"ë©”íŠ¸ë¦­ ë¡œê¹… ì‹¤íŒ¨: {e}")


# ìºì‹œ ê´€ë ¨ ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸ë“¤
@router.get(
    "/cache/statistics",
    response_model=Dict[str, Any],
    summary="ìºì‹œ í†µê³„",
    description="PostgreSQL ê¸°ë°˜ ê²€ìƒ‰ ìºì‹œì˜ ìƒì„¸ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
)
async def get_cache_statistics() -> Dict[str, Any]:
    """ìºì‹œ í†µê³„ ì¡°íšŒ"""
    try:
        stats = await search_cache_service.get_cache_statistics()
        logger.info("ìºì‹œ í†µê³„ ì¡°íšŒ ì™„ë£Œ")
        return stats
    except Exception as e:
        logger.error(f"ìºì‹œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ìºì‹œ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        )


@router.get(
    "/popular-queries",
    response_model=List[Dict[str, Any]],
    summary="ì¸ê¸° ê²€ìƒ‰ì–´",
    description="ì‚¬ìš©ìë“¤ì´ ìì£¼ ê²€ìƒ‰í•˜ëŠ” ì¸ê¸° ê²€ìƒ‰ì–´ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
)
async def get_popular_queries(
    limit: Annotated[int, Query(ge=1, le=100, description="ë°˜í™˜í•  í•­ëª© ìˆ˜")] = 20,
    min_searches: Annotated[int, Query(ge=1, description="ìµœì†Œ ê²€ìƒ‰ ìˆ˜")] = 2,
    days: Annotated[int, Query(ge=1, le=365, description="ë¶„ì„ ê¸°ê°„ (ì¼)")] = 30
) -> List[Dict[str, Any]]:
    """ì¸ê¸° ê²€ìƒ‰ì–´ ì¡°íšŒ"""
    try:
        popular_queries = await search_cache_service.get_popular_queries(
            limit=limit,
            min_searches=min_searches,
            days=days
        )
        
        logger.info(f"ì¸ê¸° ê²€ìƒ‰ì–´ ì¡°íšŒ: {len(popular_queries)}ê°œ ë°˜í™˜")
        return popular_queries
        
    except Exception as e:
        logger.error(f"ì¸ê¸° ê²€ìƒ‰ì–´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ì¸ê¸° ê²€ìƒ‰ì–´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        )


@router.get(
    "/cache/suggest",
    response_model=List[Dict[str, Any]],
    summary="ê²€ìƒ‰ì–´ ìë™ì™„ì„±",
    description="ì…ë ¥í•œ ê²€ìƒ‰ì–´ì™€ ìœ ì‚¬í•œ ìºì‹œëœ ê²€ìƒ‰ì–´ë“¤ì„ ì œì•ˆí•©ë‹ˆë‹¤."
)
async def search_suggestion(
    q: Annotated[str, Query(..., min_length=1, max_length=100, description="ê²€ìƒ‰í•  ìš©ì–´")],
    limit: Annotated[int, Query(ge=1, le=20, description="ë°˜í™˜í•  ì œì•ˆ ìˆ˜")] = 10
) -> List[Dict[str, Any]]:
    """ê²€ìƒ‰ì–´ ìë™ì™„ì„±"""
    try:
        suggestions = await search_cache_service.search_cached_queries(
            search_term=q,
            limit=limit
        )
        
        logger.debug(f"ê²€ìƒ‰ì–´ ìë™ì™„ì„±: '{q}' â†’ {len(suggestions)}ê°œ ì œì•ˆ")
        return suggestions
        
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ì–´ ìë™ì™„ì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ìë™ì™„ì„± ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        )


@router.delete(
    "/cache/invalidate",
    response_model=Dict[str, Any],
    summary="ìºì‹œ ë¬´íš¨í™”",
    description="íŠ¹ì • íŒ¨í„´ì´ë‚˜ ì „ì²´ ê²€ìƒ‰ ìºì‹œë¥¼ ë¬´íš¨í™”í•©ë‹ˆë‹¤."
)
async def invalidate_cache(
    query: Annotated[Optional[str], Query(description="íŠ¹ì • ì¿¼ë¦¬ (ì •í™• ë§¤ì¹˜)")] = None,
    pattern: Annotated[Optional[str], Query(description="ì¿¼ë¦¬ íŒ¨í„´ (ë¶€ë¶„ ë§¤ì¹˜)")] = None,
    all_cache: Annotated[bool, Query(description="ì „ì²´ ìºì‹œ ë¬´íš¨í™”")] = False
) -> Dict[str, Any]:
    """ìºì‹œ ë¬´íš¨í™”"""
    try:
        if all_cache:
            deleted_count = await search_cache_service.invalidate_cache()
        elif query:
            deleted_count = await search_cache_service.invalidate_cache(query=query)
        elif pattern:
            deleted_count = await search_cache_service.invalidate_cache(pattern=pattern)
        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="query, pattern, ë˜ëŠ” all_cache=true ì¤‘ í•˜ë‚˜ë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤"
            )
        
        logger.info(f"ìºì‹œ ë¬´íš¨í™” ì™„ë£Œ: {deleted_count}ê°œ í•­ëª© ì‚­ì œ")
        
        return {
            "success": True,
            "deleted_count": deleted_count,
            "message": f"{deleted_count}ê°œì˜ ìºì‹œ í•­ëª©ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìºì‹œ ë¬´íš¨í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ìºì‹œ ë¬´íš¨í™” ì¤‘ ì˜¤ë¥˜: {str(e)}"
        )


@router.post(
    "/cache/cleanup",
    response_model=Dict[str, Any],
    summary="ë§Œë£Œëœ ìºì‹œ ì •ë¦¬",
    description="ë§Œë£Œëœ ìºì‹œ í•­ëª©ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤. (ì¼ë°˜ì ìœ¼ë¡œ ìë™ ì‹¤í–‰ë¨)"
)
async def cleanup_expired_cache() -> Dict[str, Any]:
    """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
    try:
        cleaned_count = await search_cache_service.cleanup_expired_cache()
        
        logger.info(f"ë§Œë£Œëœ ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {cleaned_count}ê°œ í™•ì¸")
        
        return {
            "success": True,
            "cleaned_count": cleaned_count,
            "message": f"ë§Œë£Œëœ ìºì‹œ ì •ë¦¬ ì™„ë£Œ ({cleaned_count}ê°œ í™•ì¸ë¨)",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ìºì‹œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        )


# ë¼ìš°í„°ì— ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì„¤ì •
router.tags = ["Natural Language Search"]
router.responses.update({
    200: {"description": "ì„±ê³µ"},
    400: {"description": "ì˜ëª»ëœ ìš”ì²­"},
    401: {"description": "ì¸ì¦ ì‹¤íŒ¨"},
    403: {"description": "ê¶Œí•œ ì—†ìŒ"},
    404: {"description": "ë¦¬ì†ŒìŠ¤ ì—†ìŒ"},
    422: {"description": "ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨"},
    500: {"description": "ì„œë²„ ì˜¤ë¥˜"}
})