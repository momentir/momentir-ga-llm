import os
from typing import Optional
from functools import wraps
from langsmith import Client
from langchain_core.tracers.langchain import LangChainTracer
from langchain_openai import AzureChatOpenAI, ChatOpenAI
import logging

logger = logging.getLogger(__name__)

class LangSmithManager:
    """LangSmith ì¶”ì  ê´€ë¦¬ì"""
    
    def __init__(self):
        self.enabled = False
        self.client: Optional[Client] = None
        self.tracer: Optional[LangChainTracer] = None
        self.project_name = os.getenv("LANGSMITH_PROJECT", "momentir-cx-llm")
        self.llm_client = None
        
        self._initialize()
    
    def _initialize(self):
        """LangSmith ì´ˆê¸°í™”"""
        api_key = os.getenv("LANGSMITH_API_KEY")
        tracing_enabled = os.getenv("LANGSMITH_TRACING", "false").lower() == "true"
        
        if api_key and api_key != "your-langsmith-api-key-here" and tracing_enabled:
            try:
                # LangSmith í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                self.client = Client(api_key=api_key)
                
                # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
                os.environ["LANGCHAIN_TRACING_V2"] = "true"
                os.environ["LANGCHAIN_API_KEY"] = api_key
                os.environ["LANGCHAIN_PROJECT"] = self.project_name
                os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
                
                # LangChain Tracer ì´ˆê¸°í™” (ìµœì‹  ë°©ì‹)
                self.tracer = LangChainTracer()
                
                # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Azure ë˜ëŠ” OpenAI)
                self._init_llm_client()
                
                self.enabled = True
                logger.info(f"âœ… LangSmith ì¶”ì ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸: {self.project_name}")
                
            except Exception as e:
                logger.warning(f"âš ï¸  LangSmith ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.enabled = False
        else:
            logger.info("â„¹ï¸  LangSmith ì¶”ì ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    def _init_llm_client(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Azure ë˜ëŠ” OpenAI)"""
        api_type = os.getenv("OPENAI_API_TYPE", "openai")
        
        try:
            if api_type == "azure":
                self.llm_client = AzureChatOpenAI(
                    api_key=os.getenv("OPENAI_API_KEY"),
                    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
                    api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01"),
                    deployment_name=os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT_NAME", "gpt-4"),
                    callbacks=[self.tracer] if self.tracer else []
                )
                logger.info("âœ… Azure OpenAI LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self.llm_client = ChatOpenAI(
                    api_key=os.getenv("OPENAI_API_KEY"),
                    model="gpt-4",
                    callbacks=[self.tracer] if self.tracer else []
                )
                logger.info("âœ… OpenAI LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸  LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def get_callbacks(self):
        """LangChain ì½œë°± ë°˜í™˜"""
        if self.enabled and self.tracer:
            return [self.tracer]
        return []
    
    def trace_run(self, name: str, run_type: str = "llm", metadata: dict = None):
        """ì‹¤í–‰ ì¶”ì  ë°ì½”ë ˆì´í„°"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                if not self.enabled:
                    return await func(*args, **kwargs)
                
                try:
                    # ë©”íƒ€ë°ì´í„° ì„¤ì •
                    run_metadata = {
                        "function": func.__name__,
                        "module": func.__module__,
                        **(metadata or {})
                    }
                    
                    # ì‹¤í–‰ ì‹œì‘ ë¡œê¹…
                    logger.info(f"ğŸ” LangSmith ì¶”ì  ì‹œì‘: {name}")
                    
                    result = await func(*args, **kwargs)
                    
                    # ì‹¤í–‰ ì™„ë£Œ ë¡œê¹…
                    logger.info(f"âœ… LangSmith ì¶”ì  ì™„ë£Œ: {name}")
                    
                    return result
                    
                except Exception as e:
                    logger.error(f"âŒ LangSmith ì¶”ì  ì¤‘ ì˜¤ë¥˜: {name} - {e}")
                    raise
                    
            return wrapper
        return decorator
    
    def log_llm_call(self, model: str, prompt: str, response: str, metadata: dict = None):
        """LLM í˜¸ì¶œ ìˆ˜ë™ ë¡œê¹…"""
        if not self.enabled or not self.client:
            return
            
        try:
            self.client.create_run(
                name=f"llm_call_{model}",
                run_type="llm",
                inputs={"prompt": prompt},
                outputs={"response": response},
                extra={
                    "model": model,
                    **(metadata or {})
                }
            )
        except Exception as e:
            logger.warning(f"âš ï¸  LangSmith ìˆ˜ë™ ë¡œê¹… ì‹¤íŒ¨: {e}")

# ì „ì—­ LangSmith ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
langsmith_manager = LangSmithManager()

def trace_llm_call(name: str, metadata: dict = None):
    """LLM í˜¸ì¶œ ì¶”ì  ë°ì½”ë ˆì´í„°"""
    return langsmith_manager.trace_run(name, "llm", metadata)