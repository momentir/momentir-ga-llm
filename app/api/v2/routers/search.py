"""
V2 Natural Language Search Router - Next Generation AI-Powered Search

Revolutionary improvements in V2:
- Enhanced AI-powered search with multi-model ensemble
- Advanced search strategies (AI-first, Semantic Hybrid, Contextual, Adaptive, etc.)
- Real-time learning and user behavior analysis
- Context-aware search with session memory
- Improved Korean language processing
- Advanced analytics and performance monitoring
- Better error handling and user experience
"""

import logging
import asyncio
import json
from typing import List, Dict, Any, Optional, Union, Annotated
from datetime import datetime
from contextlib import asynccontextmanager

from fastapi import (
    APIRouter, Depends, HTTPException, WebSocket, WebSocketDisconnect,
    Query, Path, Body, BackgroundTasks, Request, Response, status
)
from fastapi.responses import StreamingResponse, HTMLResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, Field, ConfigDict, field_validator, computed_field
from pydantic.types import StringConstraints
from pydantic_core import ValidationError

from app.api.v2.services.nl_search_service import (
    NLSearchServiceV2, SearchStrategyV2, SearchContextV2
)
from app.api.v2.services.intent_classifier import (
    IntentClassifierV2, IntentTypeV2, IntentResultV2, ConfidenceLevelV2
)
from app.core.database import get_db
from sqlalchemy.ext.asyncio import AsyncSession

logger = logging.getLogger(__name__)

# V2 Router with enhanced capabilities
router = APIRouter(
    prefix="/v2/api/search", 
    tags=["Natural Language Search V2"],
    responses={
        404: {"description": "V2: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"},
        422: {"description": "V2: ì…ë ¥ ë°ì´í„° ê²€ì¦ ì˜¤ë¥˜"},
        500: {"description": "V2: ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜"}
    }
)

# V2 Security
security = HTTPBearer(auto_error=False)

# V2 Enhanced Type Definitions
QueryString = Annotated[str, StringConstraints(min_length=1, max_length=2000, strip_whitespace=True)]
ContextData = Annotated[Dict[str, Any], Field(default_factory=dict)]
LimitValue = Annotated[int, Field(ge=1, le=1000, default=100)]

# V2 Enhanced Request/Response Models
class SearchOptionsV2(BaseModel):
    """V2 Enhanced search options with new capabilities"""
    model_config = ConfigDict(
        str_strip_whitespace=True,
        validate_assignment=True
    )
    
    strategy: SearchStrategyV2 = Field(
        default=SearchStrategyV2.AI_FIRST,
        description="V2 ê²€ìƒ‰ ì „ëµ",
        examples=["ai_first", "semantic_hybrid", "contextual", "adaptive"]
    )
    enable_streaming: bool = Field(
        default=False,
        description="ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í™œì„±í™”"
    )
    include_explanation: bool = Field(
        default=True,
        description="AI ì¶”ë¡  ì„¤ëª… í¬í•¨"
    )
    enable_learning: bool = Field(
        default=True,
        description="ì‹¤ì‹œê°„ í•™ìŠµ í™œì„±í™”"
    )
    confidence_threshold: float = Field(
        default=0.7,
        ge=0.0,
        le=1.0,
        description="ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’"
    )
    max_suggestions: int = Field(
        default=5,
        ge=1,
        le=20,
        description="ìµœëŒ€ ì œì•ˆ ìˆ˜"
    )
    timeout_seconds: float = Field(
        default=45.0,
        ge=1.0,
        le=180.0,
        description="íƒ€ì„ì•„ì›ƒ (ì´ˆ)"
    )

class NaturalLanguageSearchRequestV2(BaseModel):
    """V2 Enhanced natural language search request"""
    model_config = ConfigDict(
        str_strip_whitespace=True,
        validate_assignment=True,
        json_schema_extra={
            "example": {
                "query": "30ëŒ€ ê³ ê°ë“¤ì˜ í‰ê·  ë³´í—˜ë£Œë¥¼ ì§€ì—­ë³„ë¡œ AI ë¶„ì„í•´ì£¼ì„¸ìš”",
                "context": {
                    "department": "analytics", 
                    "user_level": "expert",
                    "session_id": "session_123",
                    "user_preferences": {"language": "ko"}
                },
                "options": {
                    "strategy": "ai_first",
                    "include_explanation": True,
                    "enable_learning": True,
                    "confidence_threshold": 0.8
                },
                "limit": 100
            }
        }
    )
    
    query: QueryString = Field(
        ..., 
        description="V2 ìì—°ì–´ ê²€ìƒ‰ ì¿¼ë¦¬ (í–¥ìƒëœ AI ì´í•´ë ¥)",
        examples=["30ëŒ€ ê³ ê° AI ë¶„ì„", "ì§€ë‚œë‹¬ ê°€ì… íŠ¸ë Œë“œ ì˜ˆì¸¡", "ë³´í—˜ë£Œ íŒ¨í„´ í•™ìŠµ"]
    )
    context: ContextData = Field(
        default_factory=dict,
        description="V2 í–¥ìƒëœ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸",
        examples=[{"session_id": "123", "user_preferences": {"language": "ko"}}]
    )
    options: SearchOptionsV2 = Field(
        default_factory=SearchOptionsV2,
        description="V2 ê²€ìƒ‰ ì˜µì…˜"
    )
    limit: LimitValue = Field(
        default=100,
        description="ê²°ê³¼ ì œí•œ ìˆ˜"
    )
    user_id: Optional[str] = Field(
        None,
        description="ì‚¬ìš©ì ID (í•™ìŠµìš©)"
    )
    session_id: Optional[str] = Field(
        None,
        description="ì„¸ì…˜ ID (ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ìš©)"
    )

class SearchIntentV2(BaseModel):
    """V2 Enhanced search intent with multi-intent support"""
    model_config = ConfigDict(validate_assignment=True)
    
    primary_intent: str = Field(..., description="ì£¼ìš” ì˜ë„")
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="ì‹ ë¢°ë„")
    confidence_level: str = Field(..., description="ì‹ ë¢°ë„ ìˆ˜ì¤€")
    secondary_intents: List[Dict[str, Any]] = Field(default_factory=list, description="ë³´ì¡° ì˜ë„ë“¤")
    entities: Dict[str, Any] = Field(default_factory=dict, description="V2 í–¥ìƒëœ ì—”í‹°í‹°")
    context_factors: Dict[str, Any] = Field(default_factory=dict, description="ì»¨í…ìŠ¤íŠ¸ ìš”ì¸")
    intent_reasoning: str = Field(..., description="AI ì¶”ë¡  ê³¼ì •")
    suggested_actions: List[str] = Field(default_factory=list, description="ì œì•ˆëœ ì•¡ì…˜")
    uncertainty_factors: List[str] = Field(default_factory=list, description="ë¶ˆí™•ì‹¤ì„± ìš”ì¸")

class SearchExecutionV2(BaseModel):
    """V2 Enhanced search execution info"""
    model_config = ConfigDict(validate_assignment=True)
    
    search_id: str = Field(..., description="V2 ê²€ìƒ‰ ê³ ìœ  ID")
    strategy_used: str = Field(..., description="ì‚¬ìš©ëœ V2 ì „ëµ")
    processing_steps: List[str] = Field(default_factory=list, description="ì²˜ë¦¬ ë‹¨ê³„")
    ai_models_used: List[str] = Field(default_factory=list, description="ì‚¬ìš©ëœ AI ëª¨ë¸")
    execution_time_ms: float = Field(..., ge=0, description="ì‹¤í–‰ ì‹œê°„")
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="ì „ì²´ ì‹ ë¢°ë„")
    performance_metrics: Dict[str, Any] = Field(default_factory=dict, description="ì„±ëŠ¥ ì§€í‘œ")
    optimization_applied: List[str] = Field(default_factory=list, description="ì ìš©ëœ ìµœì í™”")

class NaturalLanguageSearchResponseV2(BaseModel):
    """V2 Enhanced natural language search response"""
    model_config = ConfigDict(
        validate_assignment=True,
        json_schema_extra={
            "example": {
                "search_id": "search_123456789_v2",
                "query": "30ëŒ€ ê³ ê°ë“¤ì˜ AI ë¶„ì„",
                "processed_query": {
                    "original_query": "30ëŒ€ ê³ ê°ë“¤ì˜ AI ë¶„ì„",
                    "enhanced_query": "30ëŒ€ ê³ ê°ë“¤ì˜ AI ë¶„ì„ (ì»¨í…ìŠ¤íŠ¸ ê°•í™”)",
                    "detected_language": "ko"
                },
                "intent": {
                    "primary_intent": "analytics_query",
                    "confidence_score": 0.95,
                    "intent_reasoning": "V2 AIê°€ ë¶„ì„í•œ ì˜ë„..."
                },
                "execution": {
                    "search_id": "search_123",
                    "strategy_used": "ai_first",
                    "ai_models_used": ["gpt-4", "korean_model"],
                    "confidence_score": 0.92
                },
                "results": [{"enhanced_result": "V2 ê²°ê³¼"}],
                "metadata": {
                    "total_results": 10,
                    "processing_time_seconds": 1.2,
                    "version": "2.0.0"
                }
            }
        }
    )
    
    search_id: str = Field(..., description="V2 ê²€ìƒ‰ ê³ ìœ  ID")
    query: str = Field(..., description="ì›ë³¸ ì¿¼ë¦¬")
    processed_query: Dict[str, Any] = Field(..., description="V2 ì²˜ë¦¬ëœ ì¿¼ë¦¬")
    strategy: str = Field(..., description="ì‚¬ìš©ëœ V2 ì „ëµ")
    intent: SearchIntentV2 = Field(..., description="V2 ì˜ë„ ë¶„ì„ ê²°ê³¼")
    execution: SearchExecutionV2 = Field(..., description="V2 ì‹¤í–‰ ì •ë³´")
    results: List[Dict[str, Any]] = Field(default_factory=list, description="V2 í–¥ìƒëœ ê²€ìƒ‰ ê²°ê³¼")
    metadata: Dict[str, Any] = Field(..., description="V2 ë©”íƒ€ë°ì´í„°")
    suggestions: List[str] = Field(default_factory=list, description="AI ìƒì„± ì œì•ˆ")
    success: bool = Field(..., description="ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€")
    error_message: Optional[str] = Field(None, description="ì˜¤ë¥˜ ë©”ì‹œì§€")
    timestamp: str = Field(..., description="ì‘ë‹µ ì‹œê°„")
    
    @computed_field
    @property
    def has_results(self) -> bool:
        """ê²°ê³¼ ì¡´ì¬ ì—¬ë¶€"""
        return len(self.results) > 0
    
    @computed_field
    @property
    def performance_summary(self) -> str:
        """V2 ì„±ëŠ¥ ìš”ì•½"""
        return f"V2 {self.strategy} ì „ëµìœ¼ë¡œ {self.metadata.get('processing_time_seconds', 0):.2f}ì´ˆì— {len(self.results)}ê°œ ê²°ê³¼ (ì‹ ë¢°ë„: {self.execution.confidence_score:.1%})"

# V2 Services initialization
nl_search_service_v2 = NLSearchServiceV2()
intent_classifier_v2 = IntentClassifierV2()

# V2 Enhanced API Endpoints
@router.post(
    "/natural-language",
    response_model=NaturalLanguageSearchResponseV2,
    status_code=status.HTTP_200_OK,
    summary="V2 ìì—°ì–´ ê²€ìƒ‰",
    description="""
    ## V2 ìì—°ì–´ ê²€ìƒ‰ - ì°¨ì„¸ëŒ€ AI ê²€ìƒ‰ ì—”ì§„
    
    ### ğŸš€ V2 ìƒˆë¡œìš´ ê¸°ëŠ¥
    - **ë‹¤ì¤‘ AI ëª¨ë¸ ì•™ìƒë¸”**: GPT-4, Claude, í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ì¡°í•©
    - **6ê°€ì§€ ê²€ìƒ‰ ì „ëµ**: AI-First, Semantic Hybrid, Contextual, Adaptive, Multi-Modal, Predictive
    - **ì‹¤ì‹œê°„ í•™ìŠµ**: ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ì„±ëŠ¥ ê°œì„ 
    - **ì»¨í…ìŠ¤íŠ¸ ì¸ì‹**: ì„¸ì…˜ ê¸°ë°˜ ê²€ìƒ‰ ê¸°ë¡ í™œìš©
    - **ê³ ê¸‰ ì˜ë„ ë¶„ì„**: ë‹¤ì¤‘ ì˜ë„ ë° ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
    - **ì„±ëŠ¥ ìµœì í™”**: ê³ ê¸‰ ìºì‹± ë° ì˜ˆì¸¡ì  ë¡œë”©
    
    ### ğŸ¯ V2 ê²€ìƒ‰ ì „ëµ ì„¤ëª…
    - **ai_first**: ìµœì‹  LLM ìš°ì„  (ê¸°ë³¸ê°’, ìµœê³  ì •í™•ë„)
    - **semantic_hybrid**: ì˜ë¯¸ë¡ ì  + í‚¤ì›Œë“œ í•˜ì´ë¸Œë¦¬ë“œ
    - **contextual**: ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê°œì¸í™” ê²€ìƒ‰
    - **adaptive**: ì‚¬ìš©ì í•™ìŠµ ê¸°ë°˜ ì ì‘í˜• ê²€ìƒ‰
    - **multi_modal**: ë‹¤ì¤‘ ë°ì´í„° ì†ŒìŠ¤ ìœµí•©
    - **predictive**: ì‚¬ìš©ì ì˜ë„ ì˜ˆì¸¡ ê²€ìƒ‰
    """,
    response_description="V2 í–¥ìƒëœ ê²€ìƒ‰ ê²°ê³¼ ë° AI ë¶„ì„"
)
async def natural_language_search_v2(
    request_data: NaturalLanguageSearchRequestV2,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db)
) -> NaturalLanguageSearchResponseV2:
    """V2 ìì—°ì–´ ê²€ìƒ‰ ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    
    start_time = datetime.utcnow()
    search_id = f"v2_search_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
    
    try:
        logger.info(f"ğŸ” V2 ìì—°ì–´ ê²€ìƒ‰ ì‹œì‘ [{search_id}]: {request_data.query}")
        
        # V2: 1. í–¥ìƒëœ ì˜ë„ ë¶„ì„
        intent_result = await intent_classifier_v2.classify_intent(
            text=request_data.query,
            context=request_data.context,
            user_id=request_data.user_id,
            session_id=request_data.session_id
        )
        
        logger.info(f"V2 ì˜ë„ ë¶„ì„ ì™„ë£Œ [{search_id}]: {intent_result.primary_intent.value} (ì‹ ë¢°ë„: {intent_result.confidence_score:.2f})")
        
        # V2: 2. ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        search_context = SearchContextV2()
        if request_data.user_id:
            search_context.user_id = request_data.user_id
        if request_data.session_id:
            search_context.session_id = request_data.session_id
        
        # V2: 3. í–¥ìƒëœ ìì—°ì–´ ê²€ìƒ‰ ì‹¤í–‰
        search_results = await nl_search_service_v2.search_natural_language(
            query=request_data.query,
            strategy=request_data.options.strategy,
            context=search_context,
            limit=request_data.limit,
            options=request_data.options.model_dump()
        )
        
        # V2: 4. ê²€ìƒ‰ ê²°ê³¼ì™€ ì˜ë„ ë¶„ì„ ê²°ê³¼ í†µí•©
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        # V2: Enhanced response structure
        response_data = {
            "search_id": search_id,
            "query": request_data.query,
            "processed_query": search_results.get("processed_query", {}),
            "strategy": request_data.options.strategy.value,
            "intent": SearchIntentV2(
                primary_intent=intent_result.primary_intent.value,
                confidence_score=intent_result.confidence_score,
                confidence_level=intent_result.confidence_level.value,
                secondary_intents=[
                    {"intent": intent.value, "confidence": score} 
                    for intent, score in intent_result.secondary_intents
                ],
                entities=intent_result.entities,
                context_factors=intent_result.context_factors,
                intent_reasoning=intent_result.intent_reasoning,
                suggested_actions=intent_result.suggested_actions,
                uncertainty_factors=intent_result.uncertainty_factors
            ),
            "execution": SearchExecutionV2(
                search_id=search_id,
                strategy_used=request_data.options.strategy.value,
                processing_steps=["intent_analysis", "context_building", "ai_search", "result_enhancement"],
                ai_models_used=["intent_classifier_v2", "nl_search_service_v2"],
                execution_time_ms=processing_time * 1000,
                confidence_score=search_results["metadata"].get("confidence_score", 0.0),
                performance_metrics=search_results["metadata"].get("search_quality_metrics", {}),
                optimization_applied=["v2_enhanced_processing", "context_awareness", "multi_model_ensemble"]
            ),
            "results": search_results.get("results", []),
            "metadata": {
                **search_results.get("metadata", {}),
                "intent_analysis_time_ms": intent_result.processing_metadata.get("processing_time_seconds", 0) * 1000,
                "total_processing_time_seconds": processing_time
            },
            "suggestions": search_results.get("suggestions", []),
            "success": True,
            "error_message": None,
            "timestamp": start_time.isoformat()
        }
        
        response = NaturalLanguageSearchResponseV2(**response_data)
        
        # V2: 5. ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… - í•™ìŠµ ë° ë¶„ì„
        if request_data.options.enable_learning:
            background_tasks.add_task(
                _update_learning_data_v2,
                search_id,
                request_data.query,
                response.model_dump(),
                intent_result.model_dump(),
                request_data.user_id,
                request_data.session_id
            )
        
        # V2: 6. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹…
        background_tasks.add_task(
            _log_v2_search_metrics,
            search_id,
            request_data.query,
            request_data.options.strategy.value,
            len(response.results),
            processing_time,
            response.execution.confidence_score
        )
        
        logger.info(f"âœ… V2 ê²€ìƒ‰ ì™„ë£Œ [{search_id}]: {len(response.results)}ê°œ ê²°ê³¼, {processing_time:.2f}ì´ˆ")
        return response
        
    except Exception as e:
        logger.error(f"âŒ V2 ê²€ìƒ‰ ì‹¤íŒ¨ [{search_id}]: {str(e)}")
        
        # V2: Enhanced error handling
        error_response_data = {
            "search_id": search_id,
            "query": request_data.query,
            "processed_query": {"original_query": request_data.query, "error": "processing_failed"},
            "strategy": request_data.options.strategy.value,
            "intent": SearchIntentV2(
                primary_intent="unknown",
                confidence_score=0.0,
                confidence_level="very_low",
                intent_reasoning="Error occurred during processing"
            ),
            "execution": SearchExecutionV2(
                search_id=search_id,
                strategy_used=request_data.options.strategy.value,
                processing_steps=["error_occurred"],
                ai_models_used=[],
                execution_time_ms=(datetime.utcnow() - start_time).total_seconds() * 1000,
                confidence_score=0.0,
                performance_metrics={"error": True},
                optimization_applied=[]
            ),
            "results": [],
            "metadata": {
                "total_results": 0,
                "processing_time_seconds": (datetime.utcnow() - start_time).total_seconds(),
                "version": "2.0.0",
                "error_occurred": True
            },
            "suggestions": [],
            "success": False,
            "error_message": f"V2 ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "timestamp": start_time.isoformat()
        }
        
        # Return error response instead of raising HTTP exception for better client handling
        return NaturalLanguageSearchResponseV2(**error_response_data)

@router.get(
    "/strategies/v2",
    response_model=Dict[str, Any],
    summary="V2 ê²€ìƒ‰ ì „ëµ ëª©ë¡",
    description="V2ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì°¨ì„¸ëŒ€ AI ê²€ìƒ‰ ì „ëµë“¤ê³¼ ê°ê°ì˜ íŠ¹ì§•ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
)
async def get_v2_search_strategies() -> Dict[str, Any]:
    """V2 ê²€ìƒ‰ ì „ëµ ëª©ë¡ ì¡°íšŒ"""
    
    strategies = {
        SearchStrategyV2.AI_FIRST.value: {
            "name": "AI ìš°ì„  (V2)",
            "description": "ìµœì‹  LLM ì•™ìƒë¸”ì„ í™œìš©í•œ ì§€ëŠ¥í˜• ê²€ìƒ‰",
            "accuracy": "ìµœê³ ",
            "speed": "ë¹ ë¦„",
            "cost": "ì¤‘ê°„",
            "recommended_for": ["ë³µì¡í•œ ë¶„ì„", "ì°½ì˜ì  ì¿¼ë¦¬", "ë†’ì€ ì •í™•ë„ê°€ í•„ìš”í•œ ê²½ìš°"],
            "new_features": ["ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸”", "í•œêµ­ì–´ íŠ¹í™”", "ì»¨í…ìŠ¤íŠ¸ ì¸ì‹"]
        },
        SearchStrategyV2.SEMANTIC_HYBRID.value: {
            "name": "ì˜ë¯¸ë¡ ì  í•˜ì´ë¸Œë¦¬ë“œ (V2)",
            "description": "ì˜ë¯¸ë¡ ì  ë²¡í„° ê²€ìƒ‰ê³¼ ì „í†µì  ê²€ìƒ‰ì˜ í•˜ì´ë¸Œë¦¬ë“œ",
            "accuracy": "ë†’ìŒ",
            "speed": "ì¤‘ê°„",
            "cost": "ì¤‘ê°„",
            "recommended_for": ["ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰", "ìœ ì‚¬ì„± ë¶„ì„", "ê´€ë ¨ì„± ì¤‘ì‹œ"],
            "new_features": ["ê³ ê¸‰ ì„ë² ë”©", "ì˜ë¯¸ë¡ ì  ë§¤ì¹­", "ì»¨í…ìŠ¤íŠ¸ ë²¡í„°"]
        },
        SearchStrategyV2.CONTEXTUAL.value: {
            "name": "ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ (V2)",
            "description": "ì‚¬ìš©ì ì„¸ì…˜ê³¼ ê²€ìƒ‰ ì´ë ¥ì„ í™œìš©í•œ ê°œì¸í™” ê²€ìƒ‰",
            "accuracy": "ë†’ìŒ",
            "speed": "ë¹ ë¦„",
            "cost": "ë‚®ìŒ",
            "recommended_for": ["ë°˜ë³µ ì‚¬ìš©ì", "ê°œì¸í™” í•„ìš”", "ì„¸ì…˜ ê¸°ë°˜ ì‘ì—…"],
            "new_features": ["ì„¸ì…˜ ë©”ëª¨ë¦¬", "ì‚¬ìš©ì í”„ë¡œíŒŒì¼ë§", "í–‰ë™ íŒ¨í„´ í•™ìŠµ"]
        },
        SearchStrategyV2.ADAPTIVE.value: {
            "name": "ì ì‘í˜• í•™ìŠµ (V2)",
            "description": "ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ì‹¤ì‹œê°„ í•™ìŠµ ë° ê°œì„ ",
            "accuracy": "í–¥ìƒë¨",
            "speed": "ì¤‘ê°„",
            "cost": "ì¤‘ê°„",
            "recommended_for": ["ì§€ì†ì  ì‚¬ìš©", "í”¼ë“œë°± ê¸°ë°˜ ê°œì„ ", "í•™ìŠµ í™˜ê²½"],
            "new_features": ["ì‹¤ì‹œê°„ í•™ìŠµ", "í”¼ë“œë°± í†µí•©", "ì„±ëŠ¥ ìë™ ì¡°ì •"]
        },
        SearchStrategyV2.MULTI_MODAL.value: {
            "name": "ë‹¤ì¤‘ ëª¨ë‹¬ (V2)",
            "description": "í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°, í–‰ë™ ì‹ í˜¸ë¥¼ ìœµí•©í•œ ê²€ìƒ‰",
            "accuracy": "ë§¤ìš° ë†’ìŒ",
            "speed": "ì¤‘ê°„",
            "cost": "ë†’ìŒ",
            "recommended_for": ["ë³µí•© ë°ì´í„° ë¶„ì„", "ë‹¤ì°¨ì› ê²€ìƒ‰", "ì¢…í•©ì  ì¸ì‚¬ì´íŠ¸"],
            "new_features": ["ë°ì´í„° ìœµí•©", "êµì°¨ ëª¨ë‹¬ ë§¤ì¹­", "ì¢…í•© ë¶„ì„"]
        },
        SearchStrategyV2.PREDICTIVE.value: {
            "name": "ì˜ˆì¸¡ì  ê²€ìƒ‰ (V2)",
            "description": "ì‚¬ìš©ì ì˜ë„ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ì„ ì œì  ê²°ê³¼ ì œê³µ",
            "accuracy": "ë†’ìŒ",
            "speed": "ë§¤ìš° ë¹ ë¦„",
            "cost": "ì¤‘ê°„",
            "recommended_for": ["ì˜ˆì¸¡ ë¶„ì„", "ì„ ì œì  ì •ë³´ ì œê³µ", "íš¨ìœ¨ì„± ì¤‘ì‹œ"],
            "new_features": ["ì˜ë„ ì˜ˆì¸¡", "ì„ ì œì  ë¡œë”©", "ì˜ˆì¸¡ ì•Œê³ ë¦¬ì¦˜"]
        }
    }
    
    return {
        "strategies": strategies,
        "default": SearchStrategyV2.AI_FIRST.value,
        "total_count": len(strategies),
        "v2_improvements": [
            "ë‹¤ì¤‘ AI ëª¨ë¸ ì•™ìƒë¸”",
            "ì‹¤ì‹œê°„ í•™ìŠµ ë° ì ì‘",
            "ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê°œì¸í™”",
            "ê³ ê¸‰ ì„±ëŠ¥ ìµœì í™”",
            "í–¥ìƒëœ í•œêµ­ì–´ ì§€ì›"
        ],
        "recommendation": {
            "general_use": SearchStrategyV2.AI_FIRST.value,
            "personalized": SearchStrategyV2.CONTEXTUAL.value,
            "learning_environment": SearchStrategyV2.ADAPTIVE.value,
            "best_quality": SearchStrategyV2.MULTI_MODAL.value,
            "fastest_response": SearchStrategyV2.PREDICTIVE.value
        },
        "version": "2.0.0"
    }

@router.get(
    "/analytics/v2",
    response_model=Dict[str, Any],
    summary="V2 ê²€ìƒ‰ ë¶„ì„",
    description="V2 ìì—°ì–´ ê²€ìƒ‰ ì„œë¹„ìŠ¤ì˜ ê³ ê¸‰ ë¶„ì„ ë° ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
)
async def get_v2_search_analytics() -> Dict[str, Any]:
    """V2 ê²€ìƒ‰ ë¶„ì„ ì¡°íšŒ"""
    
    try:
        # Get analytics from V2 services
        search_analytics = await nl_search_service_v2.get_search_analytics()
        intent_analytics = await intent_classifier_v2.get_classification_analytics()
        
        return {
            "version": "2.0.0",
            "timestamp": datetime.utcnow().isoformat(),
            "search_service": search_analytics,
            "intent_classifier": intent_analytics,
            "overall_performance": {
                "total_searches": search_analytics.get("total_searches", 0),
                "average_response_time": search_analytics.get("average_response_time", 0),
                "overall_accuracy": intent_analytics.get("accuracy_metrics", {}).get("overall_accuracy", 0),
                "user_satisfaction": search_analytics.get("quality_trends", {}).get("user_satisfaction", 0)
            },
            "v2_enhancements": {
                "multi_model_ensemble": True,
                "real_time_learning": True,
                "context_awareness": True,
                "korean_language_optimization": True,
                "advanced_caching": True
            }
        }
        
    except Exception as e:
        logger.error(f"V2 ë¶„ì„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"V2 ë¶„ì„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        )

@router.post(
    "/feedback/v2",
    response_model=Dict[str, Any],
    summary="V2 ê²€ìƒ‰ í”¼ë“œë°±",
    description="V2 ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•œ ì‚¬ìš©ì í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ì—¬ ì‹¤ì‹œê°„ í•™ìŠµì— í™œìš©í•©ë‹ˆë‹¤."
)
async def submit_v2_search_feedback(
    search_id: str = Field(..., description="ê²€ìƒ‰ ID"),
    rating: int = Field(..., ge=1, le=5, description="í‰ì  (1-5)"),
    feedback_type: str = Field(..., description="í”¼ë“œë°± ìœ í˜•"),
    comments: Optional[str] = Field(None, description="ì¶”ê°€ ì˜ê²¬"),
    correct_intent: Optional[str] = Field(None, description="ì˜¬ë°”ë¥¸ ì˜ë„ (ìˆ˜ì •ìš©)")
) -> Dict[str, Any]:
    """V2 ê²€ìƒ‰ í”¼ë“œë°± ì œì¶œ"""
    
    try:
        # V2: Process feedback for both services
        feedback_data = {
            "search_id": search_id,
            "rating": rating,
            "feedback_type": feedback_type,
            "comments": comments,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Update intent classifier with feedback if provided
        if correct_intent:
            try:
                correct_intent_enum = IntentTypeV2(correct_intent)
                await intent_classifier_v2.provide_feedback(
                    classification_id=search_id,
                    correct_intent=correct_intent_enum,
                    feedback_notes=comments
                )
            except ValueError:
                logger.warning(f"Invalid intent type provided: {correct_intent}")
        
        # Update search service with feedback
        optimization_result = await nl_search_service_v2.optimize_search_strategy(feedback_data)
        
        logger.info(f"V2 í”¼ë“œë°± ì²˜ë¦¬ ì™„ë£Œ: {search_id}")
        
        return {
            "success": True,
            "message": "V2 í”¼ë“œë°±ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤",
            "feedback_id": f"fb_{search_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "optimization_applied": optimization_result.get("optimization_applied", False),
            "expected_improvement": optimization_result.get("expected_improvement", 0),
            "timestamp": datetime.utcnow().isoformat(),
            "version": "2.0.0"
        }
        
    except Exception as e:
        logger.error(f"V2 í”¼ë“œë°± ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"V2 í”¼ë“œë°± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        )

@router.get(
    "/health/v2",
    response_model=Dict[str, Any],
    summary="V2 ê²€ìƒ‰ ì„œë¹„ìŠ¤ ìƒíƒœ",
    description="V2 ìì—°ì–´ ê²€ìƒ‰ ì„œë¹„ìŠ¤ì™€ ëª¨ë“  êµ¬ì„± ìš”ì†Œì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
)
async def v2_search_health_check() -> Dict[str, Any]:
    """V2 ê²€ìƒ‰ ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬"""
    
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "service": "natural_language_search_v2",
        "version": "2.0.0",
        "components": {}
    }
    
    try:
        # V2 NL Search Service ìƒíƒœ í™•ì¸
        try:
            search_context = SearchContextV2()
            test_result = await nl_search_service_v2.search_natural_language(
                query="í…ŒìŠ¤íŠ¸",
                strategy=SearchStrategyV2.AI_FIRST,
                context=search_context,
                limit=1
            )
            health_status["components"]["nl_search_service_v2"] = "healthy"
        except Exception as e:
            health_status["components"]["nl_search_service_v2"] = f"unhealthy: {str(e)}"
            health_status["status"] = "degraded"
        
        # V2 Intent Classifier ìƒíƒœ í™•ì¸
        try:
            intent_result = await intent_classifier_v2.classify_intent("í…ŒìŠ¤íŠ¸")
            health_status["components"]["intent_classifier_v2"] = "healthy"
        except Exception as e:
            health_status["components"]["intent_classifier_v2"] = f"unhealthy: {str(e)}"
            health_status["status"] = "degraded"
        
        # V2 íŠ¹í™” ê¸°ëŠ¥ ìƒíƒœ
        health_status["v2_features"] = {
            "multi_model_ensemble": "enabled",
            "real_time_learning": "enabled", 
            "context_awareness": "enabled",
            "korean_language_support": "enabled",
            "advanced_analytics": "enabled"
        }
        
        return health_status
        
    except Exception as e:
        logger.error(f"V2 í—¬ìŠ¤ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat(),
            "version": "2.0.0"
        }

# V2 Background Tasks
async def _update_learning_data_v2(
    search_id: str,
    query: str,
    search_response: Dict[str, Any],
    intent_result: Dict[str, Any],
    user_id: Optional[str],
    session_id: Optional[str]
):
    """V2 í•™ìŠµ ë°ì´í„° ì—…ë°ì´íŠ¸ (ë°±ê·¸ë¼ìš´ë“œ)"""
    try:
        logger.info(f"V2 í•™ìŠµ ë°ì´í„° ì—…ë°ì´íŠ¸: {search_id}")
        
        # TODO: Implement V2 learning data update
        # - Store search patterns
        # - Update user preferences
        # - Improve model weights
        # - Enhance context understanding
        
    except Exception as e:
        logger.error(f"V2 í•™ìŠµ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

async def _log_v2_search_metrics(
    search_id: str,
    query: str,
    strategy: str,
    result_count: int,
    duration_seconds: float,
    confidence_score: float
):
    """V2 ê²€ìƒ‰ ë©”íŠ¸ë¦­ ë¡œê¹… (ë°±ê·¸ë¼ìš´ë“œ)"""
    try:
        metrics = {
            "search_id": search_id,
            "query_length": len(query),
            "strategy_used": strategy,
            "result_count": result_count,
            "duration_seconds": duration_seconds,
            "confidence_score": confidence_score,
            "timestamp": datetime.utcnow().isoformat(),
            "version": "2.0.0"
        }
        
        logger.info(f"ğŸ“Š V2 ê²€ìƒ‰ ë©”íŠ¸ë¦­: {metrics}")
        
    except Exception as e:
        logger.error(f"V2 ë©”íŠ¸ë¦­ ë¡œê¹… ì‹¤íŒ¨: {e}")