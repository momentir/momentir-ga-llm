import os
from typing import Optional
from functools import wraps
from langsmith import Client
from langchain_core.tracers.langchain import LangChainTracer
from langchain_openai import AzureChatOpenAI, ChatOpenAI
import logging

# .env íŒŒì¼ ë¡œë“œ ì‹œë„
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

logger = logging.getLogger(__name__)

class LangSmithManager:
    """LangSmith ì¶”ì  ê´€ë¦¬ì"""
    
    def __init__(self):
        self.enabled = False
        self.client: Optional[Client] = None
        self.tracer: Optional[LangChainTracer] = None
        self.project_name = self._get_project_name()
        self.llm_client = None
        
        self._initialize()
    
    def _get_project_name(self) -> str:
        """í™˜ê²½ë³„ LangSmith í”„ë¡œì íŠ¸ëª… ê²°ì •"""
        # ìš´ì˜í™˜ê²½ ê°ì§€ ì¡°ê±´ë“¤
        is_production = (
            os.getenv("ENVIRONMENT") == "production" or 
            os.getenv("AWS_EXECUTION_ENV") is not None or
            os.getenv("ECS_CONTAINER_METADATA_URI") is not None or
            os.getenv("AWS_LAMBDA_FUNCTION_NAME") is not None
        )
        
        # ë¡œì»¬ í™˜ê²½ ëª…ì‹œì  ê°ì§€ (ìš´ì˜í™˜ê²½ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì²´í¬)
        is_local = not is_production and (
            os.getenv("ENVIRONMENT") == "local" or
            os.path.exists(".env")  # .env íŒŒì¼ ì¡´ì¬í•˜ê³  ìš´ì˜í™˜ê²½ì´ ì•„ë‹Œ ê²½ìš° ë¡œì»¬ë¡œ ê°„ì£¼
        )
        
        # ë©”ëª¨ ì •ì œìš© í”„ë¡œì íŠ¸ëª… ê²°ì •
        if is_production:
            return "momentir-cx-llm-memo"
        else:
            return "local-llm-memo"
    
    def get_excel_upload_project_name(self) -> str:
        """ì—‘ì…€ ì—…ë¡œë“œìš© í”„ë¡œì íŠ¸ëª… ë°˜í™˜"""
        # ìš´ì˜í™˜ê²½ ê°ì§€ ì¡°ê±´ë“¤
        is_production = (
            os.getenv("ENVIRONMENT") == "production" or 
            os.getenv("AWS_EXECUTION_ENV") is not None or
            os.getenv("ECS_CONTAINER_METADATA_URI") is not None or
            os.getenv("AWS_LAMBDA_FUNCTION_NAME") is not None
        )
        
        # ë¡œì»¬ í™˜ê²½ ëª…ì‹œì  ê°ì§€ (ìš´ì˜í™˜ê²½ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì²´í¬)
        is_local = not is_production and (
            os.getenv("ENVIRONMENT") == "local" or
            os.path.exists(".env")  # .env íŒŒì¼ ì¡´ì¬í•˜ê³  ìš´ì˜í™˜ê²½ì´ ì•„ë‹Œ ê²½ìš° ë¡œì»¬ë¡œ ê°„ì£¼
        )
        
        # ì—‘ì…€ ì—…ë¡œë“œìš© í”„ë¡œì íŠ¸ëª… ê²°ì •
        if is_production:
            return "momentir-cx-llm-excel-upload"
        else:
            return "local-llm-excel-upload"
    
    def _initialize(self):
        """LangSmith ì´ˆê¸°í™”"""
        api_key = os.getenv("LANGSMITH_API_KEY")
        tracing_enabled = os.getenv("LANGSMITH_TRACING", "false").lower() == "true"
        
        if api_key and api_key != "your-langsmith-api-key-here" and tracing_enabled:
            try:
                # LangSmith í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                self.client = Client(api_key=api_key)
                
                # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
                os.environ["LANGCHAIN_TRACING_V2"] = "true"
                os.environ["LANGCHAIN_API_KEY"] = api_key
                os.environ["LANGCHAIN_PROJECT"] = self.project_name
                os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
                
                # LangChain Tracer ì´ˆê¸°í™” (ìµœì‹  ë°©ì‹)
                self.tracer = LangChainTracer()
                
                # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Azure ë˜ëŠ” OpenAI)
                self._init_llm_client()
                
                self.enabled = True
                logger.info(f"âœ… LangSmith ì¶”ì ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸: {self.project_name}")
                
            except Exception as e:
                logger.warning(f"âš ï¸  LangSmith ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.enabled = False
        else:
            logger.info("â„¹ï¸  LangSmith ì¶”ì ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    def _init_llm_client(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Azure ë˜ëŠ” OpenAI)"""
        api_type = os.getenv("OPENAI_API_TYPE", "openai")
        
        try:
            if api_type == "azure":
                self.llm_client = AzureChatOpenAI(
                    api_key=os.getenv("OPENAI_API_KEY"),
                    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
                    api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01"),
                    deployment_name=os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT_NAME", "gpt-4"),
                    callbacks=[self.tracer] if self.tracer else []
                )
                logger.info("âœ… Azure OpenAI LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                self.llm_client = ChatOpenAI(
                    api_key=os.getenv("OPENAI_API_KEY"),
                    model="gpt-4",
                    callbacks=[self.tracer] if self.tracer else []
                )
                logger.info("âœ… OpenAI LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸  LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def get_callbacks(self, project_name: Optional[str] = None):
        """LangChain ì½œë°± ë°˜í™˜ (í”„ë¡œì íŠ¸ë³„ ì„¤ì • ì§€ì›)"""
        if self.enabled and self.tracer:
            # í”„ë¡œì íŠ¸ëª…ì´ ì§€ì •ëœ ê²½ìš°, ìƒˆë¡œìš´ tracer ìƒì„±
            if project_name:
                # í”„ë¡œì íŠ¸ëª…ì„ ì§ì ‘ ì§€ì •í•˜ì—¬ tracer ìƒì„±
                temp_tracer = LangChainTracer(project_name=project_name)
                return [temp_tracer]
            return [self.tracer]
        return []
    
    def trace_run(self, name: str, run_type: str = "llm", metadata: dict = None, project_name: Optional[str] = None):
        """ì‹¤í–‰ ì¶”ì  ë°ì½”ë ˆì´í„° (í”„ë¡œì íŠ¸ë³„ ì„¤ì • ì§€ì›)"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                if not self.enabled:
                    return await func(*args, **kwargs)
                
                try:
                    # ë©”íƒ€ë°ì´í„° ì„¤ì •
                    run_metadata = {
                        "function": func.__name__,
                        "module": func.__module__,
                        "project": project_name or self.project_name,
                        **(metadata or {})
                    }
                    
                    # ì‹¤í–‰ ì‹œì‘ ë¡œê¹…
                    used_project = project_name or self.project_name
                    logger.info(f"ğŸ” LangSmith ì¶”ì  ì‹œì‘: {name} (í”„ë¡œì íŠ¸: {used_project})")
                    
                    # í”„ë¡œì íŠ¸ëª…ì´ ì§€ì •ëœ ê²½ìš°, í™˜ê²½ë³€ìˆ˜ ì„ì‹œ ë³€ê²½
                    original_project = None
                    if project_name:
                        original_project = os.environ.get("LANGCHAIN_PROJECT")
                        os.environ["LANGCHAIN_PROJECT"] = project_name
                    
                    try:
                        result = await func(*args, **kwargs)
                    finally:
                        # ì›ë˜ í™˜ê²½ë³€ìˆ˜ ë³µì›
                        if project_name and original_project:
                            os.environ["LANGCHAIN_PROJECT"] = original_project
                    
                    # ì‹¤í–‰ ì™„ë£Œ ë¡œê¹…
                    logger.info(f"âœ… LangSmith ì¶”ì  ì™„ë£Œ: {name} (í”„ë¡œì íŠ¸: {used_project})")
                    
                    return result
                    
                except Exception as e:
                    logger.error(f"âŒ LangSmith ì¶”ì  ì¤‘ ì˜¤ë¥˜: {name} - {e}")
                    raise
                    
            return wrapper
        return decorator
    
    def log_llm_call(self, model: str, prompt: str, response: str, metadata: dict = None, project_name: Optional[str] = None):
        """LLM í˜¸ì¶œ ìˆ˜ë™ ë¡œê¹… (í”„ë¡œì íŠ¸ë³„ ì„¤ì • ì§€ì›)"""
        if not self.enabled or not self.client:
            return
            
        try:
            # í”„ë¡œì íŠ¸ëª…ì´ ì§€ì •ëœ ê²½ìš° í™˜ê²½ë³€ìˆ˜ ì„ì‹œ ë³€ê²½
            original_project = None
            if project_name:
                original_project = os.environ.get("LANGCHAIN_PROJECT")
                os.environ["LANGCHAIN_PROJECT"] = project_name
            
            try:
                self.client.create_run(
                    name=f"llm_call_{model}",
                    run_type="llm",
                    inputs={"prompt": prompt},
                    outputs={"response": response},
                    extra={
                        "model": model,
                        "project": project_name or self.project_name,
                        **(metadata or {})
                    }
                )
            finally:
                # ì›ë˜ í™˜ê²½ë³€ìˆ˜ ë³µì›
                if project_name and original_project:
                    os.environ["LANGCHAIN_PROJECT"] = original_project
                    
        except Exception as e:
            logger.warning(f"âš ï¸  LangSmith ìˆ˜ë™ ë¡œê¹… ì‹¤íŒ¨: {e}")

# ì „ì—­ LangSmith ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
langsmith_manager = LangSmithManager()

def trace_llm_call(name: str, metadata: dict = None, project_name: Optional[str] = None):
    """LLM í˜¸ì¶œ ì¶”ì  ë°ì½”ë ˆì´í„° (í”„ë¡œì íŠ¸ë³„ ì„¤ì • ì§€ì›)"""
    return langsmith_manager.trace_run(name, "llm", metadata, project_name)

def trace_excel_upload_call(name: str, metadata: dict = None):
    """ì—‘ì…€ ì—…ë¡œë“œ ê´€ë ¨ LLM í˜¸ì¶œ ì¶”ì  ë°ì½”ë ˆì´í„°"""
    project_name = langsmith_manager.get_excel_upload_project_name()
    return langsmith_manager.trace_run(name, "llm", metadata, project_name)

def get_excel_upload_llm_client():
    """ì—‘ì…€ ì—…ë¡œë“œ ì „ìš© LangChain í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜ (ì—‘ì…€ ì—…ë¡œë“œ í”„ë¡œì íŠ¸ë¡œ ì„¤ì •ë¨)"""
    if not langsmith_manager.enabled:
        return None
    
    # ì—‘ì…€ ì—…ë¡œë“œ í”„ë¡œì íŠ¸ëª… ê°€ì ¸ì˜¤ê¸°
    excel_project = langsmith_manager.get_excel_upload_project_name()
    
    # ì—‘ì…€ ì—…ë¡œë“œ ì „ìš© ì½œë°± ìƒì„±
    callbacks = langsmith_manager.get_callbacks(excel_project)
    
    # API íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    api_type = os.getenv("OPENAI_API_TYPE", "openai")
    
    try:
        if api_type == "azure":
            from langchain_openai import AzureChatOpenAI
            return AzureChatOpenAI(
                api_key=os.getenv("OPENAI_API_KEY"),
                azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
                api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01"),
                deployment_name=os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT_NAME", "gpt-4"),
                callbacks=callbacks
            )
        else:
            from langchain_openai import ChatOpenAI
            return ChatOpenAI(
                api_key=os.getenv("OPENAI_API_KEY"),
                model="gpt-4",
                callbacks=callbacks
            )
    except Exception as e:
        logger.warning(f"âš ï¸  ì—‘ì…€ ì—…ë¡œë“œ ì „ìš© LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return None